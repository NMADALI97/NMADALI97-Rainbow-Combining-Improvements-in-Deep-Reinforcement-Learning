{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rainbow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVe-Ell9Nv7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"Archive.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HpLcvfrTEXF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d543ac17-6143-45ed-c85c-9d5620ebd71c"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA8MmKzDR8fF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class obj(object):\n",
        "    def __init__(self, d):\n",
        "        for a, b in d.items():\n",
        "            if isinstance(b, (list, tuple)):\n",
        "               setattr(self, a, [obj(x) if isinstance(x, dict) else x for x in b])\n",
        "            else:\n",
        "               setattr(self, a, obj(b) if isinstance(b, dict) else b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0fgRQSLOXch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import torch\n",
        "\n",
        "    # Basic Arguments\n",
        "d  = {'seed': 1122,\n",
        "    'batch_size':32,\n",
        "    'no_cuda':False,\n",
        "    # Training Arguments\n",
        "    'max_frames':1400000,\n",
        "                        \n",
        "    'buffer_size':100000,\n",
        "                       \n",
        "    'update_target':1000,\n",
        "                       \n",
        "    'train_freq':1,\n",
        "                       \n",
        "    'gamma':0.99,\n",
        "           \n",
        "    'learning_start':10000,\n",
        "                        \n",
        "    'eps_start':1.0,\n",
        "                       \n",
        "    'eps_final':0.01,\n",
        "                       \n",
        "    'eps_decay':30000,\n",
        "                    \n",
        "    # Algorithm Arguments\n",
        "    'double':True, #Enable Double_Q Learning\n",
        "    'dueling':True, #Enable Dueling Network\n",
        "    'noisy':True, #Enable Noisy Network\n",
        "    'prioritized_replay' :True, #enable prioritized experience replay\n",
        "    'c51':True, #enable categorical dqn\n",
        "    'multi_step':1,\n",
        "                       \n",
        "    'Vmin':-10 , #Minimum value of support for c51\n",
        "    'Vmax':10,  #Maximum value of support for c51\n",
        "    'num_atoms': 51,# Number of atom for c51\n",
        "    'alpha' : 0.6, #Alpha value for prioritized replay\n",
        "    'beta_start':0.4,#Start value of beta for prioritized replay\n",
        "    'beta_frames':100000,#End frame of beta schedule for prioritized replay\n",
        "    'sigma_init':0.4,#Sigma initialization value for NoisyNet\n",
        "\n",
        "    # Environment Arguments\n",
        "    'env':'PongNoFrameskip-v4', #Environment Name\n",
        "    'episode_life':1, #Whether env has episode life(1) or not(0)\n",
        "    'clip_rewards':1,#Whether env clip rewards(1) or not(0)\n",
        "    'frame_stack':1,#Whether env stacks frame(1) or not(0)\n",
        "    'scale':0,#Whether env scales(1) or not(0)\n",
        "\n",
        "    # Evaluation Arguments\n",
        "    'load_model':None, #Pretrained model name to load (state dict)\n",
        "    'save_model':'model', #Pretrained model name to save (state dict)\n",
        "    'evaluate':None, #'Evaluate only'\n",
        "    'render':None ,#Render evaluation agent\n",
        "    'evaluation_interval': 10000,#Frames for evaluation interval\n",
        "\n",
        "    # Optimization Arguments\n",
        "    'lr':1e-4 ,#'Learning rate'\n",
        "    \n",
        "    'cuda' : torch.cuda.is_available(),\n",
        "    'device' : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "}\n",
        " \n",
        "parms = obj(d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yylzre9S19A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "import time, os\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from common.utils import create_log_dir, print_args, set_global_seeds\n",
        "from common.wrappers import make_atari, wrap_atari_dqn\n",
        "#from arguments import get_args\n",
        "#from train import train\n",
        "#from test import test\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHpTOPjFTNI9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "outputId": "39b31fca-aef6-4fb8-a501-9d2cc8b3c74a"
      },
      "source": [
        "print_args(parms)\n",
        "log_dir = create_log_dir(parms)\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "env = make_atari(parms.env)\n",
        "env = wrap_atari_dqn(env, parms)\n",
        "\n",
        "set_global_seeds(parms.seed)\n",
        "env.seed(parms.seed)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          Options\n",
            "                          seed: 1122\n",
            "                          batch_size: 32\n",
            "                          no_cuda: False\n",
            "                          max_frames: 1400000\n",
            "                          buffer_size: 100000\n",
            "                          update_target: 1000\n",
            "                          train_freq: 1\n",
            "                          gamma: 0.99\n",
            "                          learning_start: 10000\n",
            "                          eps_start: 1.0\n",
            "                          eps_final: 0.01\n",
            "                          eps_decay: 30000\n",
            "                          double: True\n",
            "                          dueling: True\n",
            "                          noisy: True\n",
            "                          prioritized_replay: True\n",
            "                          c51: True\n",
            "                          multi_step: 1\n",
            "                          Vmin: -10\n",
            "                          Vmax: 10\n",
            "                          num_atoms: 51\n",
            "                          alpha: 0.6\n",
            "                          beta_start: 0.4\n",
            "                          beta_frames: 100000\n",
            "                          sigma_init: 0.4\n",
            "                          env: PongNoFrameskip-v4\n",
            "                          episode_life: 1\n",
            "                          clip_rewards: 1\n",
            "                          frame_stack: 1\n",
            "                          scale: 0\n",
            "                          load_model: None\n",
            "                          save_model: model\n",
            "                          evaluate: None\n",
            "                          render: None\n",
            "                          evaluation_interval: 10000\n",
            "                          lr: 0.0001\n",
            "                          cuda: True\n",
            "                          device: cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1122, 1711756444]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwzMpi1BWp0m",
        "colab_type": "text"
      },
      "source": [
        "#Train "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAd0hWM1U-_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time, os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "from common.utils import epsilon_scheduler, beta_scheduler, update_target, print_log, load_model, save_model\n",
        "from model import DQN\n",
        "from common.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
        "\n",
        "def compute_td_loss(current_model, target_model, replay_buffer, optimizer, args, beta=None):\n",
        "    \"\"\"\n",
        "    Calculate loss and optimize for non-c51 algorithm\n",
        "    \"\"\"\n",
        "    if args.prioritized_replay:\n",
        "        state, action, reward, next_state, done, weights, indices = replay_buffer.sample(args.batch_size, beta)\n",
        "    else:\n",
        "        state, action, reward, next_state, done = replay_buffer.sample(args.batch_size)\n",
        "        weights = torch.ones(args.batch_size)\n",
        "\n",
        "    state = torch.FloatTensor(np.float32(state)).to(args.device)\n",
        "    next_state = torch.FloatTensor(np.float32(next_state)).to(args.device)\n",
        "    action = torch.LongTensor(action).to(args.device)\n",
        "    reward = torch.FloatTensor(reward).to(args.device)\n",
        "    done = torch.FloatTensor(done).to(args.device)\n",
        "    weights = torch.FloatTensor(weights).to(args.device)\n",
        "\n",
        "    if not args.c51:\n",
        "        q_values = current_model(state)\n",
        "        target_next_q_values = target_model(next_state)\n",
        "\n",
        "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        if args.double:\n",
        "            next_q_values = current_model(next_state)\n",
        "            next_actions = next_q_values.max(1)[1].unsqueeze(1)\n",
        "            next_q_value = target_next_q_values.gather(1, next_actions).squeeze(1)\n",
        "        else:\n",
        "            next_q_value = target_next_q_values.max(1)[0]\n",
        "\n",
        "        expected_q_value = reward + (args.gamma ** args.multi_step) * next_q_value * (1 - done)\n",
        "\n",
        "        loss = F.smooth_l1_loss(q_value, expected_q_value.detach(), reduction='none')\n",
        "        if args.prioritized_replay:\n",
        "            prios = torch.abs(loss) + 1e-5\n",
        "        loss = (loss * weights).mean()\n",
        "    \n",
        "    else:\n",
        "        q_dist = current_model(state)\n",
        "        action = action.unsqueeze(1).unsqueeze(1).expand(args.batch_size, 1, args.num_atoms)\n",
        "        q_dist = q_dist.gather(1, action).squeeze(1)\n",
        "        q_dist.data.clamp_(0.01, 0.99)\n",
        "\n",
        "        target_dist = projection_distribution(current_model, target_model, next_state, reward, done, \n",
        "                                              target_model.support, target_model.offset, args)\n",
        "\n",
        "        loss = - (target_dist * q_dist.log()).sum(1)\n",
        "        if args.prioritized_replay:\n",
        "            prios = torch.abs(loss) + 1e-6\n",
        "        loss = (loss * weights).mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    if args.prioritized_replay:\n",
        "        replay_buffer.update_priorities(indices, prios.data.cpu().numpy())\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def projection_distribution(current_model, target_model, next_state, reward, done, support, offset, args):\n",
        "    delta_z = float(args.Vmax - args.Vmin) / (args.num_atoms - 1)\n",
        "\n",
        "    target_next_q_dist = target_model(next_state)\n",
        "\n",
        "    if args.double:\n",
        "        next_q_dist = current_model(next_state)\n",
        "        next_action = (next_q_dist * support).sum(2).max(1)[1]\n",
        "    else:\n",
        "        next_action = (target_next_q_dist * support).sum(2).max(1)[1]\n",
        "\n",
        "    next_action = next_action.unsqueeze(1).unsqueeze(1).expand(target_next_q_dist.size(0), 1, target_next_q_dist.size(2))\n",
        "    target_next_q_dist = target_next_q_dist.gather(1, next_action).squeeze(1)\n",
        "\n",
        "    reward = reward.unsqueeze(1).expand_as(target_next_q_dist)\n",
        "    done = done.unsqueeze(1).expand_as(target_next_q_dist)\n",
        "    support = support.unsqueeze(0).expand_as(target_next_q_dist)\n",
        "\n",
        "    Tz = reward + args.gamma * support * (1 - done)\n",
        "    Tz = Tz.clamp(min=args.Vmin, max=args.Vmax)\n",
        "    b = (Tz - args.Vmin) / delta_z\n",
        "    l = b.floor().long()\n",
        "    u = b.ceil().long()\n",
        "\n",
        "    target_dist = target_next_q_dist.clone().zero_()\n",
        "    target_dist.view(-1).index_add_(0, (l + offset).view(-1), (target_next_q_dist * (u.float() - b)).view(-1))\n",
        "    target_dist.view(-1).index_add_(0, (u + offset).view(-1), (target_next_q_dist * (b - l.float())).view(-1))\n",
        "\n",
        "    return target_dist\n",
        "\n",
        "def multi_step_reward(rewards, gamma):\n",
        "    ret = 0.\n",
        "    for idx, reward in enumerate(rewards):\n",
        "        ret += reward * (gamma ** idx)\n",
        "    return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j41rIAjbTfHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "current_model = DQN(env, parms).to(parms.device)\n",
        "target_model = DQN(env, parms).to(parms.device)\n",
        "\n",
        "if parms.noisy:\n",
        "        current_model.update_noisy_modules()\n",
        "        target_model.update_noisy_modules()\n",
        "\n",
        "if parms.load_model and os.path.isfile(parms.load_model):\n",
        "        load_model(current_model, parms)\n",
        "\n",
        "epsilon_by_frame = epsilon_scheduler(parms.eps_start, parms.eps_final, parms.eps_decay)\n",
        "beta_by_frame = beta_scheduler(parms.beta_start, parms.beta_frames)\n",
        "\n",
        "if parms.prioritized_replay:\n",
        "        replay_buffer = PrioritizedReplayBuffer(parms.buffer_size, parms.alpha)\n",
        "else:\n",
        "        replay_buffer = ReplayBuffer(parms.buffer_size)\n",
        "    \n",
        "state_deque = deque(maxlen=parms.multi_step)\n",
        "reward_deque = deque(maxlen=parms.multi_step)\n",
        "action_deque = deque(maxlen=parms.multi_step)\n",
        "\n",
        "optimizer = optim.Adam(current_model.parameters(), lr=parms.lr)\n",
        "\n",
        "reward_list, length_list, loss_list = [], [], []\n",
        "episode_reward = 0\n",
        "episode_length = 0\n",
        "\n",
        "prev_time = time.time()\n",
        "prev_frame = 1\n",
        "\n",
        "state = env.reset()\n",
        "for frame_idx in range(1, parms.max_frames + 1):\n",
        "        if parms.render:\n",
        "            env.render()\n",
        "\n",
        "        if parms.noisy:\n",
        "            current_model.sample_noise()\n",
        "            target_model.sample_noise()\n",
        "\n",
        "        epsilon = epsilon_by_frame(frame_idx)\n",
        "        action = current_model.act(torch.FloatTensor(state).to(parms.device), epsilon)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state_deque.append(state)\n",
        "        reward_deque.append(reward)\n",
        "        action_deque.append(action)\n",
        "\n",
        "        if len(state_deque) == parms.multi_step or done:\n",
        "            n_reward = multi_step_reward(reward_deque, parms.gamma)\n",
        "            n_state = state_deque[0]\n",
        "            n_action = action_deque[0]\n",
        "            replay_buffer.push(n_state, n_action, n_reward, next_state, np.float32(done))\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        episode_length += 1\n",
        "\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            reward_list.append(episode_reward)\n",
        "            length_list.append(episode_length)\n",
        "            writer.add_scalar(\"data/episode_reward\", episode_reward, frame_idx)\n",
        "            writer.add_scalar(\"data/episode_length\", episode_length, frame_idx)\n",
        "            episode_reward, episode_length = 0, 0\n",
        "            state_deque.clear()\n",
        "            reward_deque.clear()\n",
        "            action_deque.clear()\n",
        "\n",
        "        if len(replay_buffer) > parms.learning_start and frame_idx % parms.train_freq == 0:\n",
        "            beta = beta_by_frame(frame_idx)\n",
        "            loss = compute_td_loss(current_model, target_model, replay_buffer, optimizer, parms, beta)\n",
        "            loss_list.append(loss.item())\n",
        "            writer.add_scalar(\"data/loss\", loss.item(), frame_idx)\n",
        "\n",
        "        if frame_idx % parms.update_target == 0:\n",
        "            update_target(current_model, target_model)\n",
        "\n",
        "        if frame_idx % parms.evaluation_interval == 0:\n",
        "            print_log(frame_idx, prev_frame, prev_time, reward_list, length_list, loss_list)\n",
        "            reward_list.clear(), length_list.clear(), loss_list.clear()\n",
        "            prev_frame = frame_idx\n",
        "            prev_time = time.time()\n",
        "            save_model(current_model, parms)\n",
        "\n",
        "save_model(current_model, parms)\n",
        "\n",
        "writer.export_scalars_to_json(os.path.join(log_dir, \"all_scalars.json\"))\n",
        "writer.close()\n",
        "env.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdKLIeI-WlO2",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h5jrIKFWRc_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b108f497-d2c4-42b1-e6db-059378691b82"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import os\n",
        "from common.utils import load_model\n",
        "from model import DQN\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "current_model = DQN(env, parms).to(parms.device)\n",
        "current_model.eval()\n",
        "\n",
        "load_model(current_model, parms)\n",
        "\n",
        "episode_reward = 0\n",
        "episode_length = 0\n",
        "\n",
        "state = env.reset()\n",
        "frames = []\n",
        "while True:\n",
        "        if parms.render:\n",
        "          \n",
        "            env.render()\n",
        "        frames.append(Image.fromarray(env.render(mode='rgb_array')))  \n",
        "        action = current_model.act(torch.FloatTensor(state).to(parms.device), 0.)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        episode_length += 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "print(\"Test Result - Reward {} Length {}\".format(episode_reward, episode_length))\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Result - Reward 20.0 Length 1669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F6dudLEbFCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('openai_gym.gif', 'wb') as f:  # change the path if necessary\n",
        "    im = Image.new('RGB', frames[0].size)\n",
        "    im.save(f, save_all=True, append_images=frames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orYixDbNbYs9",
        "colab_type": "text"
      },
      "source": [
        "<img id=\"gif\" src=\"\"/>\n",
        "\n",
        "<script>document.getElementById(\"gif\".src=\"openai_gym.gif?\"+new Date().getTime();</script>"
      ]
    }
  ]
}